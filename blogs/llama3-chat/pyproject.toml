[project]
dependencies = [ "gradio>=5.8.0,<6", "fastapi>=0.115.6,<0.116", "requests>=2.32.3,<3", "openai>=1.57.3,<2", "tenacity>=9.0.0,<10", "transformers>=4.47.0,<5", "click>=8.1.7,<9"]
description = "Chat with llama3 MAX Serve on GPU"
name = "llama3-chat"
requires-python = ">= 3.9,<3.13"
version = "0.0.0"

[build-system]
build-backend = "hatchling.build"
requires = ["hatchling"]

[tool.hatch.build.targets.wheel]
packages = ["."]

[tool.pixi.project]
channels = ["conda-forge", "https://conda.modular.com/max"]
platforms = ["linux-64", "osx-arm64", "linux-aarch64"]

[tool.pixi.pypi-dependencies]
llama3_chat = { path = ".", editable = true }

[tool.pixi.tasks]
